# loss = nan

训练深度学习网路的时候出现Nan是什么原因，怎么避免？下面列举一下可能出现的原因以及解决办法

梯度爆炸或者不收敛，都是表象，出现了loss=nan，就说明模型发散，跑飞了，此时应该停止训练查找原因。总的原因可以分为以下三个方面，需要根据自己的经验一一查找：

1. 数据问题
1. 网络设计
1. 硬件原因
1. 代码问题

出现这个问题，可以这样操作检查，本地检查数据，服务器修改学习速率等简单参数，两个都没问题的话，修改数据处理方法以及检查网络结构。代码问题不太好检查，如果是用的公开的代码，可以去github的iuuse上提问，如果自己重写的代码需要一点点调试
  
## 数据问题

### 脏数据

这个查找起来比较麻烦，有一下解决办法供参考：

1. 最不合理的数据是数据本身中有nan，这个必须首先检查一下
1. 最费劲的办法是把batch_size设置为1，shuffle=false，然后挨个检查定位
1. 在tensorflow中如果出现
    ```bash
    RuntimeWarning: invaid value encountered in log targets_dw=np.log(gt_widths/ex_widths)
    ```
    * 说明数据出现问题，多出现在么有控制好边界  
        例如coco数据集是从xmin和ymin都是从1开始的，所以在tensorflow中pascal_voc.py中有一个-1的操作，如果自己制作的数据是从0开始的，-1就会出现log(-1)的情况，log(0)=inf，log(-1)无意义  
    * 数据越界有6中情况：x1<0;  x2>width;  x2<x1;  y1<0;  y2>height;  y2<y1，都需要考虑
1. 最好的解决办法是清晰确定程序的数据处理过程，然后有针对性的处理数据
1. 除了log函数，exp函数也会产生nan

### label不对

本来有5000维label，设置成4999，这样就不对了

### 网络结构

如果从训练一开始就出现nan，推测是网络设计的问题，一般都是首先测试经典的网络，如果是自己重新设计的网络，可以考虑以下方法

1. 弱化场景，将样本简化，学习率等参数都设置为典型配置，比如10万个样本都是同一张图像，让网路去拟合，如果还出现问题就说明网络设计不合理，否则就是参数问题
1. 网络问题，可以不断加大样本的复杂度和调整网络来改变。有可能网络后面层的参数更新异常，增加后面层的宽度试试
1. 参数微调，在网络的你和能力和样本复杂度匹配的情况下，是可以train到一定的水平然后进一步调优。weights的可视化也可以是微调中使用的方法，digits tf中都有相关的工具

### 数据归一化

减均值除方差，或者加入normalization例如BN、L2 norm等

### 学习速率lr过大

学习率比较大的时候，参数可能over shoot了，结果找不到极小值点，减小学习率可以让参数朝着极值点前进 
每个层都可以设置学习率，也可以尝试减小后面层的学习率。
这个是指标不治本的方法，不过是把报错的时间推迟罢了，而且学习率过低的话，本身就就有很大风险陷入局部最优。

### 更新参数初始化方法

现在训练网络一般都是要有pre-trained model，所以这一条暂时不用考虑

### 针对梯度爆炸

参考斯坦福CS 224D的lecture note，可以加入gradient clipping，每当梯度达到一定的阈值，就把他们设置回一个小一些的数字

### tensorflow中交叉熵

交叉熵`cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))`的话，最后softmax层输出y_conv的取值范围在[0,1]页就是说允许取0值，有log(0)出现很有可能出现nan啊，`cross_entropy = -tf.reduce_mean(y_*tf.log(tf.clip_by_value(y_conv,1e-15,1.0)))`在tensorflow里可以限定一下y_conv的取值范围

### 检查特征

用keras训练的时候也遇到了loss忽然降为nan的情况，最后发现是有一列特征太不发散，做了标准化之后基本全为0的缘故，删掉该列

## 硬件问题

### gpu的arch设置的不对

修改arch的设置与电脑的算力相当，[查看算力](https://developer.nvidia.com/cuda-gpus)
